<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Caching vs. Database Replication Strategies in Chained MR Jobs</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/projects.css">
    <link rel="stylesheet" href="css/demo.css">
    <link rel="stylesheet" href="css/profiles.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.10/css/all.css" integrity="sha384-+d0P83n9kaQMCwj8F4RJB66tzIwOKmrdb46+porD/OvrJ+37WqIM7UoBtwHO6Nlg" crossorigin="anonymous">
  </head>
  <body>
    <div class="content">
      <div class="card" id="about">
        <div class="col-md-12 col-xs-12">
          <h1>Caching vs. Database Replication Strategies in Chained MR Jobs</h1>
          <h2 class="colorful">Team Members</h2>
          <div class="profile-container">
            <div class="profile">
              <div>
                <img src="img/andrew.jpeg" alt="picture">
                <h1>Andrew Wong</h1>
                <h3>BS, Computer Science '19</h3>
              </div>
              <div class="social">
                <a href="https://github.com/andrewwong97/"><i class="fab fa-github"></i></a>
              </div>
            </div>
            <div class="profile">
              <div>
                <img src="https://avatars1.githubusercontent.com/u/17332287?s=460&v=4" alt="picture">
                <h1>Ryan Demo</h1>
                <h3>MSE, Computer Science '19<br>BS, Electrical Eng. & CS '18</h3>
              </div>
              <div class="social">
                <a href="https://github.com/ryandemo/"><i class="fab fa-github"></i></a>
              </div>
            </div>
            <div class="profile">
              <div>
                <img src="img/ben.jpg" alt="picture">
                <h1>Benjamin Hoertnagl-Pereira</h1>
                <h3>MSE, Computer Science '19<br>BS, Computer Eng. '18</h3>
              </div>
              <div class="social">
                <a href="https://github.com/bpereira615"><i class="fab fa-github"></i></a>
              </div>
            </div>
            <div class="profile">
              <div>
                <img src="https://avatars2.githubusercontent.com/u/6342492?s=460&v=4" alt="picture">
                <h1>Daniel Sohn </h1>
                <h3>BS, Computer Science & AMS '20</h3>
              </div>
              <div class="social">
                <a href="https://github.com/dsohn45/"><i class="fab fa-github"></i></a>
              </div>
            </div>
          </div>
        </div>
        <h3>Problem Statement</h3>
        <p>We are looking to investigate strategies for database caching and replication depending on the data workload. Specifically, for a tenant in a datacenter that has to compute on large amounts of uniformly sampled data queried from a database, where first read time can add latency, which combination of local caching and database replication minimizes job compute times?</p>
        <h3>Experimental Background</h3>
        <p>In chained MapReduce jobs in Hadoop that read randomly, uniformly sampled documents from a database after each reduce (such as hash ids), looking up hash pointers in the database is dependent on the ids and an appropriate database sharding + replication scheme is necessary to reduce job times with this workload.</p>
        <p>Some real world tasks for this are traversing a graph with hash references to nodes, looking up digital signature metadata for multiple rounds of signing, and calculating the lowest degree vertex in a graph.</p>
        <h3>Experiment</h3>
        <p>Our baseline will measure job time using one database node and 32 compute nodes all sending it queries with no compute node local caches. The database node will have an in-memory integrated cache.</p>
        <p>The independent variables are:</p>
        <ul>
          <li><p>Database size: 4 GB, 16 GB, 64 GB</p></li>
          <li><p>Number of shards: 1, 2, 4, 8</p></li>
          <li><p>Number of replicas per shard: 1, 2</p></li>
          <li><p>MapReduce Chain Length: 1, 2, 4, 8</p></li>
          <li><p>Size of local caches for queried data on compute nodes: 0 GB, 0.5 GB, 1 GB, 2 GB</p></li>
          <li><p>Job interarrival profile: bursty, normally distributed</p></li>
        </ul>
        <p>The measured dependent variables are:</p>
        <ul>
          <li><p>Elapsed (real/wall) time for job completion</p></li>
          <li><p>CPU time (user + sys) and peak memory usage per compute node and per database node</p></li>
        </ul>
        <p>From all permutations, we will look at results to determine best practices for caching and/or replication depending on mainly the data compute workload, and analyzing the performance results of each strategy.</p>
        <p>If the previous results prove conclusive, we will develop a simple query load balancer to that dynamically adjusts the amount of database nodes containing shards/replicas based on heuristics from static results, and evaluate its performance compared to the static amount of nodes.</p>
        <a href="https://github.com/andrewwong97/cloud-computing"><button class="slide-deck"><i class="fab fa-github" style="color:#fff;margin-right:7px;"></i>Code</button></a>
        <a href="https://docs.google.com/presentation/d/1_1JsO5GTepdY9hRmS98DQsO2Ny2-NclOpoOS0PtP8rg/edit?usp=sharing"><button class="slide-deck">Midterm Slides</button></a>
        <h3>Timeline</h3>
        <ul>
          <li><p>March 4: Project Proposal</p></li>
          <li><p>March 16: Literature Review</p></li>
          <li><p>March 23: Problem Refinement</p></li>
          <li><p>March 30: Experimental Refinement, Checkpoint 1</p></li>
          <li><p>April 8: Experiment Setup, Midterm Presentation</p></li>
          <li><p>April 21: Start Experiment Execution, Checkpoint 2</p></li>
          <li><p>April 24: Finish Experiment Execution</p></li>
          <li><p>May 1: Write Dynamic Load Balancer and Evaluate It</p></li>
          <li><p>May 4: Results and Paper Completion</p></li>
        </ul>
        <h3>Results (Work in Progress)</h3>
        <p>We are currently running and refining experiments, and expect to have all the data by 4/24. Delays in collection have been due to modeling the data for our workload, testing the code and getting the database cluster to behave the way we want it to.</p>
        <p>Initial results with 100 database keys, 1-2 shards, and a Redis cache size of 256MB show that sharding without Redis has increasingly worse performance as the MR chain grows in length. We will let this inform the experiment parameter permutations that we focus on in the short term.</p>
        <a href="https://docs.google.com/document/d/1sSOxL6yRWxurd6sCd54RiuN8viMvkDu4GQ-vu953i_Q/edit"><button class="slide-deck">Unformatted Results</button></a>
        <a href="https://docs.google.com/spreadsheets/d/1fDwYwueKhE5hdaxhqf5m62Psqo0PluMvmk-dVW2a5oE/edit?usp=sharing"><button class="slide-deck">Spreadsheet</button></a>
        <h3>Existing Work</h3>
        <a href="http://www.pdl.cmu.edu/PDL-FTP/CloudComputing/p236-xiao-SoCC15.pdf">ShardFS vs. IndexFS: Replication vs. Caching Strategies for Distributed Metadata Management in Cloud Storage Systems</a>
        <p>The rapid growth of cloud storage systems calls for fast and scalable namespace processing. While few commercial file systems offer anything better than federating individually non-scalable namespace servers, a recent academic file system, IndexFS, demonstrates scalable namespace processing based on client caching of directory entries and permissions (directory lookup state) with no per-client state in servers. In this paper we explore explicit replication of directory lookup state in all servers as an alternative to caching this information in all clients. Both eliminate most repeated RPCs to different servers in order to resolve hierarchical permission tests. Our realization for server replicated directory lookup state, ShardFS, employs a novel file system specific hybrid optimistic and pessimistic concurrency control favoring single object transactions over distributed transactions. Our experimentation suggests that if directory lookup state mutation is a fixed fraction of operations (strong scaling for metadata), server replication does not scale as well as client caching, but if directory lookup state mutation is proportional to the number of jobs, not the number of processes per job, (weak scaling for metadata), then server replication can scale more linearly than client caching and provide lower 70 percentile response times as well.</p>
        <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.2100&rep=rep1&type=pdf">Analysis of Caching and Replication Strategies for Web Applications</a>
        <p>Replication and caching mechanisms are often employed to enhance the performance of Web applications. In this article, we present a qualitative and quantitative analysis of state-of-the-art replication and caching techniques used to host Web applications. Our analysis shows that the selection of best mechanism is heavily dependant on the data workload and requires careful analysis of the application characteristics. To this end, we propose a technique that may enable future Web practitioners to compare the performance of different caching/replication mechanisms.</p>
        <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5795483/pdf/sensors-18-00222.pdf">Replication Strategy for Spatiotemporal Data Based on Distributed Caching System</a>
        <p>The replica strategy in distributed cache can effectively reduce user access delay and improve system performance. However, developing a replica strategy suitable for varied application scenarios is still quite challenging, owing to differences in user access behavior and preferences. In this paper, a replication strategy for spatiotemporal data (RSSD) based on a distributed caching system is proposed. By taking advantage of the spatiotemporal locality and correlation of user access, RSSD mines high popularity and associated files from historical user access information, and then generates replicas and selects appropriate cache node for placement. Experimental results show that the RSSD algorithm is simple and efficient, and succeeds in significantly reducing user access delay.</p>
        <h3>Extra Notes</h3>
        <p>Database sharding and replication best practices for when data is poorly cacheable or requires fast first reads, and example applications that would use this paradigm.</p>
        <p>Cloud providers that offer multi-tenant services must handle compute hotspots reliably in order to optimize resource usage. Usually the problem is that the server lacks the CPU/memory to execute all the queries, or there is a bandwidth issue. Even if these are hardware limitations and not necessarily database dependent, it can be solved by sharding/replication either by dividing up the queries to different shards or moving a copy of the database closer. One approach to handle dynamic compute-heavy localities relying on the same data is via distributed database sharding.  For example, compute-heavy jobs requiring access to the same data shards may be bottlenecked by database reads, where network congestion (bandwidth) or CPU can be limiting factors of how fast requests are processed.</p>
        <p>To have enough copies of relevant data to balance the compute load across multiple nodes, the database can be replicated dynamically. It is the conditions under which replication should occur and the extent to which it should occur that we are interested in.</p>
        <p>In context of the CAP theorem, there may be multiple optimal replication strategies based on the priorities of the distributed database. For example, given different loads such as read-heavy, mixed r/w, write-heavy operations, caching might be better in read-heavy situations, while sharding might be better when the data is updated frequently. </p>
        <h3>Checkpoint 1 Updates</h3>
        <ul>
          <li><p>Analysis of existing systems - Andrew, Ryan, Ben, Daniel</p></li>
          <li><p>Experiment specification - Ryan, Daniel</p></li>
          <li><p>Google Cloud VM configuration - Andrew</p></li>
          <li><p>Project homepage - Andrew, Ben</p></li>
        </ul>
        <h3>Checkpoint 2 Updates</h3>
        <ul>
          <li><p>Website updates - Andrew, Ryan</p></li>
          <li><p>Mongo cluster setup - Andrew</p></li>
          <li><p>Database updates - Andrew, Daniel</p></li>
          <li><p>Cache layer (Redis) implementation - Andrew</p></li>
          <li><p>MR Job Code - Ben, Daniel</p></li>
          <li><p>Resource Measurement Code - Ryan, Daniel</p></li>
          <li><p>Experimental Evaluation - Ben, Ryan, Daniel</p></li>
        </ul>
        <p>For this iteration we refined our Google Cloud MongoDB cluster with the following architecture: 1 MapReduce node, 1 mongos app router, 1 config server, 2 shard servers. Each regular node uses the default Debian 9 node in Google Cloud Compute Engine with 3.75 GB of memory and 1 vCPU. Shard nodes: 1.7 GB of memory, 1 vCPU. The mongos app router is an interface to two eponymous databases: single and cloud, where single holds a single collection of data on the app router node, and cloud holds the sharded collection of data on the shard servers. The config server exists to hold metadata and shard lookup information for the app router during queries. In addition to the architecture, we set up the /etc/hosts file on each node so each node within a cluster has easy aliased access to each other. Finally, we wrote a data ingress pipeline so we can easily add and remove data when tweaking cluster configuration parameters in future iterations.</p>
        <p>We ran experiments on a 4GB sized database with a normally distributed interarrival profile, 1 replica per shard, 1-2 shards, and varying cache sizes from 0-2GB. The workload was a MR chain of 5 jobs. These can be found in the results spreadsheet.</p>
        <p>We also set up the results spreadsheet to know exactly the data we're capturing, and have it generating charts as we input data.</p>
        <h3>Next Steps</h3>
        <ul>
          <li><p>Measurement analysis</p></li>
          <li><p>Formal written results analysis</p></li>
          <li><p>Heuristics code</p></li>
          <li><p>Dynamic replica and shard creation code run by controller</p></li>
          <li><p>Final paper</p></li>
        </ul>

      </div>
      <!-- <div class="card emo">
        <div class="description">
          <h1>Repository</h1>
          <a href="https://github.com/andrewwong97/cloud-computing"
             class="btn-github"
          >
            Github <i class="fab fa-github"></i>
          </a>
          <h2>Includes how to build the system</h2>
          <a href="https://github.com/andrewwong97/cloud-computing"><button class="btn-more">Demo</button></a>
        </div>
        <div class="preview">
          <img src="https://user-images.githubusercontent.com/7339169/55187736-275b6980-5170-11e9-8d45-4c9a5d0e0d41.png">
        </div>
      </div> -->
    </div>
  </body>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.min.js"></script>
</html>
